{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL3KIKrzvjcd"
      },
      "source": [
        "### Word2Vec Implementation from Scratch\n",
        "\n",
        "# Introduction\n",
        " Word2Vec is a popular technique for word embeddings, which captures the meaning of words by placing them in a continuous vector space.\n",
        " In this exercise, you will implement Word2Vec using NumPy and complete the missing parts of the code.\n",
        "We will represent each word as a one-hot vector, meaning each word in the vocabulary is mapped to a unique binary vector with only one active (1) position."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gpCfRgivsvG"
      },
      "source": [
        "import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O5uKnkUJvJDq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from numpy.linalg import norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dzazztv_wtbI"
      },
      "outputs": [],
      "source": [
        "### Adjust the hyperparameters if needed ###\n",
        "settings = {\n",
        "\t'window_size': 2,\n",
        "\t'n': 10,\n",
        "\t'epochs': 50,\n",
        "\t'learning_rate': 0.01\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "GlpwAgpnv6tY"
      },
      "outputs": [],
      "source": [
        "class word2vec:\n",
        "    def __init__(self, settings):\n",
        "        \"\"\"\n",
        "        Initialize the Word2Vec model with given hyperparameters.\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        self.n = settings['n']\n",
        "        self.lr = settings['learning_rate']\n",
        "        self.epochs = settings['epochs']\n",
        "        self.window = settings['window_size']\n",
        "        ## End code\n",
        "\n",
        "    def generate_training_data(self, corpus):\n",
        "        \"\"\"\n",
        "        Generate training data from the given corpus.\n",
        "        This function processes the input corpus to create training examples for the Word2Vec model.\n",
        "        - It first counts the occurrences of each word in the corpus.\n",
        "        - Then, it creates a vocabulary of unique words and assigns each word a unique index.\n",
        "        - Finally, it generates training pairs consisting of target words and their surrounding context words.\n",
        "        \"\"\"\n",
        "        # ## Start code\n",
        "        # word_counts = np.unique(corpus)\n",
        "        # self.words_list = word_counts\n",
        "        self.words_list = list(set(corpus))\n",
        "        self.v_count = len(self.words_list)\n",
        "        #self.v_count = len(word_counts)\n",
        "        self.word_index ={word: i for i, word in enumerate(self.words_list)}\n",
        "        self.index_word = {i: word for i, word in enumerate(self.words_list)}\n",
        "        #training_data = corpus\n",
        "        training_data = []\n",
        "        for i, word in enumerate(corpus):\n",
        "            start = max(0, i - self.window)\n",
        "            end = min(len(corpus), i + self.window + 1)\n",
        "\n",
        "            for j in range(start, end):\n",
        "                if j != i:\n",
        "                    training_data.append((word, corpus[j]))\n",
        "        return training_data\n",
        "        ## End code\n",
        "\n",
        "    def word2onehot(self, word):\n",
        "        \"\"\"\n",
        "        Convert a word into a one-hot encoded vector.\n",
        "        Output:\n",
        "        - A one-hot vector of length equal to the vocabulary size.\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        word_vec = np.zeros(self.v_count)\n",
        "        word_index = self.word_index[word]\n",
        "        word_vec[word_index] = 1\n",
        "        return word_vec\n",
        "        ## End code\n",
        "\n",
        "    def train(self, training_data):\n",
        "        \"\"\"\n",
        "        Train the model using the given training data.\n",
        "        This function initializes the weight matrices and performs forward and backward propagation.\n",
        "        - Initializes weight matrices w1 (input to hidden) and w2 (hidden to output)\n",
        "        - Iterates through training data and performs forward pass\n",
        "        - Computes the error and updates weights using backpropagation\n",
        "        - Tracks and prints loss for each epoch\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        self.w1 = np.random.uniform(-1,1,(self.v_count,self.n))\n",
        "        self.w2 = np.random.uniform(-1,1,(self.n,self.v_count))\n",
        "        losses = []\n",
        "        for i in range(self.epochs):\n",
        "            self.loss = 0\n",
        "            for target_word, context_word in training_data:\n",
        "                x = self.word2onehot(target_word)\n",
        "                y_true = self.word2onehot(context_word)\n",
        "                #y_true[self.word_index[w]] = 1\n",
        "                y_pred, h, u = self.forward_pass(x)\n",
        "                error = y_true - y_pred\n",
        "                self.backprop(error, h, x)\n",
        "                # self.loss += -np.sum([u[word] for word in training_data[i] if word in self.word_index])\n",
        "                # self.loss += np.sum([u[word] for word in training_data[i] if word in self.word_index])\n",
        "                #self.loss += -np.sum(y_true * np.log(y_pred))\n",
        "                epsilon = 1e-10\n",
        "                self.loss -= np.sum(y_true * np.log(y_pred + epsilon))\n",
        "            if i % 5 == 0:\n",
        "              print('Epoch:', i, \"Loss:\", self.loss)\n",
        "        ## End code\n",
        "\n",
        "    def softmax(self, x):\n",
        "        \"\"\"\n",
        "        Apply softmax function.\n",
        "        This function normalizes the input values into probabilities, ensuring that they sum to 1.\n",
        "        - It exponentiates each value in x to ensure non-negativity.\n",
        "        - It divides each exponentiated value by the sum of all exponentiated values to normalize them into a probability distribution.\n",
        "        Output:\n",
        "        - A probability distribution where the sum of all elements equals 1.\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        # e_x = np.exp(x)/np.sum(np.exp(x))\n",
        "        # return e_x\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum()\n",
        "\n",
        "        ## End code\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        This function takes a one-hot encoded word vector as input and performs the following steps:\n",
        "        - Computes the hidden layer by multiplying the input vector with the first weight matrix.\n",
        "        - Computes the output layer values by multiplying the hidden layer with the second weight matrix.\n",
        "        - Applies the softmax function to get the probability distribution over the vocabulary.\n",
        "        Output:\n",
        "        - The predicted probability distribution (y_c), hidden layer activations (h), and raw scores before softmax (u).\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        # self.h = np.dot(self.W.T,X).reshape(self.N,1)\n",
        "        # h = np.dot(self.w1.T,x)\n",
        "        # u = np.dot(self.w2.T,h)\n",
        "        h = np.dot(x, self.w1)\n",
        "        u = np.dot(h, self.w2)\n",
        "        y_c = self.softmax(u)\n",
        "        return y_c, h, u\n",
        "        ## End code\n",
        "\n",
        "    def backprop(self, e, h, x):\n",
        "        \"\"\"\n",
        "        Backpropagation step to update weights.\n",
        "        This function updates the weight matrices using gradient descent.\n",
        "        - Computes the gradient of the loss with respect to the second weight matrix (w2).\n",
        "        - Computes the gradient of the loss with respect to the first weight matrix (w1).\n",
        "        - Updates w1 and w2 using the learning rate and computed gradients.\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        dl_dw2 = np.outer(h,e)\n",
        "        #dl_dw1 = np.outer(x,np.dot(self.w2,e))\n",
        "        dl_dw1 = np.outer(x, np.dot(self.w2, e))\n",
        "        # self.w1 += self.lr * dl_dw1\n",
        "        # self.w2 += self.lr * dl_dw2\n",
        "        self.w2 -= self.lr * dl_dw2\n",
        "        self.w1 -= self.lr * dl_dw1\n",
        "        ## End code\n",
        "\n",
        "    def word_vec(self, word):\n",
        "        \"\"\"\n",
        "        Retrieve the word vector for a given word.\n",
        "        \"\"\"\n",
        "        ## Start code\n",
        "        # v_w = self.w1[self.word_index[word]]\n",
        "        # return v_w\n",
        "        if word in self.word_index:\n",
        "            return self.w1[self.word_index[word]]\n",
        "        else:\n",
        "            return None\n",
        "        ## End code\n",
        "    def vec_sim(self, word, top_n):\n",
        "        \"\"\"\n",
        "        Find top N most similar words based on cosine similarity,\n",
        "        excluding the word itself.\n",
        "        \"\"\"\n",
        "        if word not in self.word_index:\n",
        "            return []\n",
        "\n",
        "        word_idx = self.word_index[word]\n",
        "        target_vector = self.w1[word_idx]\n",
        "\n",
        "        dot_products = np.dot(self.w1, target_vector)\n",
        "        word_norms = np.linalg.norm(self.w1, axis=1)\n",
        "        target_norm = np.linalg.norm(target_vector)\n",
        "        similarities = dot_products / (word_norms * target_norm)\n",
        "\n",
        "        # Set similarity with itself to -1 so it won't be in top N\n",
        "        similarities[word_idx] = -1\n",
        "\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_n]\n",
        "        return [(self.index_word[idx], float(similarities[idx])) for idx in top_indices]\n",
        "\n",
        "\n",
        "        # v_w1 = self.word_vec(word)\n",
        "        # word_vector_norm = norm(v_w1)\n",
        "        # all_norms = np.array([norm(self.w1[i]) for i in range(self.v_count)])\n",
        "        # # word_sim = np.dot(self.w1,v_w1)\n",
        "        # # word_sim /= np.linalg.norm(self.w1, axis=1) * np.linalg.norm(v_w1)\n",
        "        # # words_sorted = np.argsort(-word_sim)\n",
        "        # similarities = []\n",
        "        # for i in range(self.v_count):\n",
        "        #     #v_w2 = self.w1[i]\n",
        "        #     cos_sim = np.dot(v_w1, self.w1[i]) / (word_vector_norm * all_norms[i])\n",
        "        #     similarities.append((self.index_word[i], cos_sim))\n",
        "\n",
        "        ## End code\n",
        "        #return [(self.index_word[i], word_sim[i]) for i in words_sorted[:top_n]]\n",
        "        # return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "OrARMd5i6Zqt"
      },
      "outputs": [],
      "source": [
        "text = \"Natural language processing and machine learning open up fascinating possibilities, allowing machines to analyze,\\\n",
        " understand, and respond to human language in ways that were once thought impossible.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Q7ntj1Es6u2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "692f5542-af3b-419d-f483-267c881840d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 399.58849521914567\n",
            "Epoch: 5 Loss: 514.2201209836763\n",
            "Epoch: 10 Loss: 858.3782907378849\n",
            "Epoch: 15 Loss: 1655.2511233078117\n",
            "Epoch: 20 Loss: 2212.6405590357963\n",
            "Epoch: 25 Loss: 2290.113983751027\n",
            "Epoch: 30 Loss: 2300.7763996859135\n",
            "Epoch: 35 Loss: 2285.8998097167223\n",
            "Epoch: 40 Loss: 2279.559242063803\n",
            "Epoch: 45 Loss: 2279.559242063803\n"
          ]
        }
      ],
      "source": [
        "corpus = [word.lower() for word in text.split()]\n",
        "\n",
        "# tokens = [' ', '.', ',',\"?\"]\n",
        "# for i in range (len(corpus[0])):\n",
        "#   if corpus[0][i][-1] in tokens:\n",
        "#     corpus[0][i] = corpus[0][i][:-1]\n",
        "\n",
        "w2v = word2vec(settings)\n",
        "\n",
        "training_data = w2v.generate_training_data(corpus)\n",
        "\n",
        "w2v.train(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "fPbTmdZu6uJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebeb47b-fe9e-4cf6-985a-6888cb6673a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "machine [ 33.61616145 -42.0168299  -55.81369974 -13.77658166 -11.48039286\n",
            "   3.0925903   -6.399108   -33.32873671   4.62688482 -24.98989089]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fascinating', 0.9787328739701204),\n",
              " ('were', 0.9610179561940602),\n",
              " ('that', 0.9436386448237125),\n",
              " ('impossible.', 0.9314480889784025),\n",
              " ('language', 0.8981565681780966)]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "word = \"machine\"\n",
        "vec = w2v.word_vec(word)\n",
        "print(word, vec)\n",
        "\n",
        "# Find similar words\n",
        "w2v.vec_sim(\"machine\", 5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}